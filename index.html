<!DOCTYPE html><html lang="en" class="scroll-smooth"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="image" href="/bio.jpg"/><link rel="stylesheet" href="/_next/static/css/bd781d6ed6b0325b.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-679b75d1c4c2027c.js"/><script src="/_next/static/chunks/4bd1b696-70b6d399998de86a.js" async=""></script><script src="/_next/static/chunks/684-5aaa8290a129f299.js" async=""></script><script src="/_next/static/chunks/main-app-dcb36f35b86b1673.js" async=""></script><script src="/_next/static/chunks/17-9c04c9413a9f9f1f.js" async=""></script><script src="/_next/static/chunks/473-8ffb501d6ec7a482.js" async=""></script><script src="/_next/static/chunks/app/layout-2cd749d8c5f06c63.js" async=""></script><script src="/_next/static/chunks/178-595a94b9af1e67b5.js" async=""></script><script src="/_next/static/chunks/748-1f3129a1e6365cf9.js" async=""></script><script src="/_next/static/chunks/app/page-4a023a9e3aac6a3b.js" async=""></script><link rel="icon" href="/favicon.svg" type="image/svg+xml"/><link rel="dns-prefetch" href="https://google-fonts.jialeliu.com"/><link rel="preconnect" href="https://google-fonts.jialeliu.com" crossorigin=""/><link rel="preload" as="style" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/><link rel="stylesheet" id="gfonts-css" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap" media="print"/><script>
              (function(){
                var l = document.getElementById('gfonts-css');
                if (!l) return;
                if (l.media !== 'all') {
                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });
                }
              })();
            </script><noscript><link rel="stylesheet" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/></noscript><script>
              try {
                const theme = localStorage.getItem('theme-storage');
                const parsed = theme ? JSON.parse(theme) : null;
                const setting = parsed?.state?.theme || 'system';
                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));
                var root = document.documentElement;
                root.classList.add(effective);
                root.setAttribute('data-theme', effective);
              } catch (e) {
                var root = document.documentElement;
                root.classList.add('light');
                root.setAttribute('data-theme', 'light');
              }
            </script><title>Binzhu Xie</title><meta name="description" content="PhD student at The Chinese University of Hong Kong."/><meta name="author" content="Binzhu Xie"/><meta name="keywords" content="Binzhu Xie,PhD,Research,The Chinese University of Hong Kong."/><meta name="creator" content="Binzhu Xie"/><meta name="publisher" content="Binzhu Xie"/><meta property="og:title" content="Binzhu Xie"/><meta property="og:description" content="PhD student at The Chinese University of Hong Kong."/><meta property="og:site_name" content="Binzhu Xie&#x27;s Academic Website"/><meta property="og:locale" content="en_US"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Binzhu Xie"/><meta name="twitter:description" content="PhD student at The Chinese University of Hong Kong."/><link rel="icon" href="/favicon.svg"/><script>document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="font-sans antialiased"><div style="visibility:hidden"><nav class="fixed top-0 left-0 right-0 z-50" data-headlessui-state=""><div class="transition-all duration-300 ease-out bg-transparent" style="transform:translateY(-100px)"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex justify-between items-center h-16 lg:h-20"><div class="flex-shrink-0" tabindex="0"><a class="text-xl lg:text-2xl font-serif font-semibold text-primary hover:text-accent transition-colors duration-200" href="/">Binzhu Xie</a></div><div class="hidden lg:block"><div class="ml-10 flex items-center space-x-8"><div class="flex items-baseline space-x-8"><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-primary" href="/#about"><span class="relative z-10">About</span><div class="absolute inset-0 bg-accent/10 rounded-lg"></div></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/#news"><span class="relative z-10">News</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/#featured_publications"><span class="relative z-10">Selected Publications</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/#services"><span class="relative z-10">Services</span></a></div><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div></div></div><div class="lg:hidden flex items-center space-x-2"><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div><button class="inline-flex items-center justify-center p-2 rounded-md text-neutral-600 hover:text-primary hover:bg-neutral-100 dark:hover:bg-neutral-800 focus:outline-none focus:ring-2 focus:ring-inset focus:ring-accent transition-colors duration-200" id="headlessui-disclosure-button-¬´R5pdb¬ª" type="button" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Open main menu</span><div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="block h-6 w-6"><path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg></div></button></div></div></div></div></nav><main class="min-h-screen pt-16 lg:pt-20"><div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen"><div class="grid grid-cols-1 lg:grid-cols-3 gap-12"><div class="lg:col-span-1"><div class="sticky top-8" style="opacity:0;transform:translateY(20px)"><div class="w-40 h-40 mx-auto mb-6 rounded-2xl overflow-hidden shadow-lg hover:shadow-xl transition-all duration-200 hover:scale-105"><img alt="Binzhu Xie" width="160" height="160" decoding="async" data-nimg="1" class="w-full h-full object-cover object-[32%_center]" style="color:transparent" src="/bio.jpg"/></div><div class="text-center mb-6"><h1 class="text-3xl font-serif font-bold text-primary mb-2">Binzhu Xie</h1><p class="text-lg text-accent font-medium mb-1">Ph.D. Student @ CUHK</p><p class="text-neutral-600 mb-2">The Chinese University of Hong Kong.</p></div><div class="flex flex-wrap justify-center gap-3 sm:gap-4 mb-6 relative px-2"><div class="relative"><button class="p-2 sm:p-2 transition-colors duration-200 text-neutral-600 dark:text-neutral-400 hover:text-accent" aria-label="Email"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M21.75 6.75v10.5a2.25 2.25 0 0 1-2.25 2.25h-15a2.25 2.25 0 0 1-2.25-2.25V6.75m19.5 0A2.25 2.25 0 0 0 19.5 4.5h-15a2.25 2.25 0 0 0-2.25 2.25m19.5 0v.243a2.25 2.25 0 0 1-1.07 1.916l-7.5 4.615a2.25 2.25 0 0 1-2.36 0L3.32 8.91a2.25 2.25 0 0 1-1.07-1.916V6.75"></path></svg></button></div><div class="relative"><button class="p-2 sm:p-2 transition-colors duration-200 text-neutral-600 dark:text-neutral-400 hover:text-accent" aria-label="Location"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M15 10.5a3 3 0 1 1-6 0 3 3 0 0 1 6 0Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 10.5c0 7.142-7.5 11.25-7.5 11.25S4.5 17.642 4.5 10.5a7.5 7.5 0 1 1 15 0Z"></path></svg></button></div><a href="https://scholar.google.com/citations?user=yFREVP0AAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="Google Scholar"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M4.26 10.147a60.438 60.438 0 0 0-.491 6.347A48.62 48.62 0 0 1 12 20.904a48.62 48.62 0 0 1 8.232-4.41 60.46 60.46 0 0 0-.491-6.347m-15.482 0a50.636 50.636 0 0 0-2.658-.813A59.906 59.906 0 0 1 12 3.493a59.903 59.903 0 0 1 10.399 5.84c-.896.248-1.783.52-2.658.814m-15.482 0A50.717 50.717 0 0 1 12 13.489a50.702 50.702 0 0 1 7.74-3.342M6.75 15a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm0 0v-3.675A55.378 55.378 0 0 1 12 8.443m-7.007 11.55A5.981 5.981 0 0 0 6.75 15.75v-1.5"></path></svg></a><a href="https://github.com/Nicous20" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-github h-5 w-5" aria-hidden="true"><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"></path><path d="M9 18c-4.51 2-5-2-7-2"></path></svg></a><a href="https://www.linkedin.com/in/binzhu-xie-bb2b04298/" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="LinkedIn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-linkedin h-5 w-5" aria-hidden="true"><path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect width="4" height="12" x="2" y="9"></rect><circle cx="4" cy="4" r="2"></circle></svg></a></div><div class="bg-neutral-100 dark:bg-neutral-800 rounded-lg p-4 mb-6 hover:shadow-lg transition-all duration-200 hover:scale-[1.02]"><h3 class="font-semibold text-primary mb-3">Research Interests</h3><div class="space-y-2 text-sm text-neutral-700 dark:text-neutral-500"><div>3D Vision (highly related to XR and HCI)</div><div>Visual Multimodally Understanding</div><div>AIGC related topic</div></div></div><div class="flex justify-center"><div class="relative"><button class="flex items-center space-x-2 px-4 py-2 rounded-lg font-medium text-sm transition-all duration-200 bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-500 hover:bg-red-50 dark:hover:bg-red-900/20 hover:text-red-600 dark:hover:text-red-400 cursor-pointer" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-4 w-4"><path stroke-linecap="round" stroke-linejoin="round" d="M21 8.25c0-2.485-2.099-4.5-4.688-4.5-1.935 0-3.597 1.126-4.312 2.733-.715-1.607-2.377-2.733-4.313-2.733C5.1 3.75 3 5.765 3 8.25c0 7.22 9 12 9 12s9-4.78 9-12Z"></path></svg><span>Like</span></button></div></div></div></div><div class="lg:col-span-2 space-y-8"><section id="about" class="scroll-mt-24 space-y-8"><section style="opacity:0;transform:translateY(20px)"><h2 class="text-2xl font-serif font-bold text-primary mb-4">About</h2><div class="text-neutral-700 dark:text-neutral-600 leading-relaxed"><p class="mb-4 last:mb-0">Binzhu Xie (Ë∞¢Êª®Á´π) is a first year PhD student at the <a href="https://www.cse.cuhk.edu.hk/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Department of Computer Science and Engineering</a>, <a href="https://www.cuhk.edu.hk/chinese/index.html" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">The Chinese University of Hong Kong (CUHK)</a>, advised by <a href="https://shiqiu0419.github.io/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Prof. Shi QIU</a> at the CUHK‚Äôs Institute of Medical Intelligence and XR, led by <a href="https://www.cse.cuhk.edu.hk/~pheng/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Prof. Pheng Ann Heng</a>. Previously, Binzhu Xie obtained his bachelor in 2020 from a <a href="https://www.qmul.ac.uk/global/partnerships/jointprogrammes/bupt/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">joint program</a> between <a href="https://www.bupt.edu.cn/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">BUPT</a> and <a href="https://www.qmul.ac.uk/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">QMUL</a>, advised by <a href="https://scholar.google.com/citations?user=uSykWkMAAAAJ&amp;hl=en" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Prof. Guoshun Nan</a>. Since 2023, He has been fortunate to collaborate with <a href="https://www.mmlab-ntu.com/index.html" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">MMLab@NTU</a>, working closely with <a href="https://jingkang50.github.io/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Dr. Jingkang Yang</a> under the supervision of <a href="https://liuziwei7.github.io/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Prof. Ziwei Liu</a>.</p></div></section><div id="news"><section style="opacity:0;transform:translateY(20px)"><h2 class="text-2xl font-serif font-bold text-primary mb-4">News</h2><div class="relative"><div class="space-y-3 max-h-70 overflow-y-auto pr-2 scrollbar-thin scrollbar-thumb-neutral-300 dark:scrollbar-thumb-neutral-700 scrollbar-track-transparent"><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2026-01</span><div class="text-sm text-neutral-700 prose prose-sm dark:prose-invert max-w-none"><p node="[object Object]"><a href="https://arxiv.org/abs/2601.19850" node="[object Object]" class="text-accent hover:text-accent-dark underline-offset-2 underline" target="_blank" rel="noopener noreferrer">EgoHandICL</a> accepted to ICLR 2026.</p></div></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2026-01</span><div class="text-sm text-neutral-700 prose prose-sm dark:prose-invert max-w-none"><p node="[object Object]">Joined CUHK CSE as a PhD student! üéâ</p></div></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2025-07</span><div class="text-sm text-neutral-700 prose prose-sm dark:prose-invert max-w-none"><p node="[object Object]"><a href="https://arxiv.org/abs/2510.11830" node="[object Object]" class="text-accent hover:text-accent-dark underline-offset-2 underline" target="_blank" rel="noopener noreferrer">One paper</a> accepted to ACM MM 2025.</p></div></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2025-06</span><div class="text-sm text-neutral-700 prose prose-sm dark:prose-invert max-w-none"><p node="[object Object]"><a href="https://arxiv.org/abs/2507.22100" node="[object Object]" class="text-accent hover:text-accent-dark underline-offset-2 underline" target="_blank" rel="noopener noreferrer">TRIG</a> accepted to ICCV 2025.</p></div></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2025-03</span><div class="text-sm text-neutral-700 prose prose-sm dark:prose-invert max-w-none"><p node="[object Object]">Two papers accepted to CVPR 2025. <a href="https://egolife-ai.github.io/" node="[object Object]" class="text-accent hover:text-accent-dark underline-offset-2 underline" target="_blank" rel="noopener noreferrer">EgoLife</a> is fantastic! üï∂Ô∏è</p></div></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2025-01</span><div class="text-sm text-neutral-700 prose prose-sm dark:prose-invert max-w-none"><p node="[object Object]"><a href="https://arxiv.org/abs/2501.09302" node="[object Object]" class="text-accent hover:text-accent-dark underline-offset-2 underline" target="_blank" rel="noopener noreferrer">One work</a> accepted as IEEE VR 2025 Posters.</p></div></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2024-11</span><div class="text-sm text-neutral-700 prose prose-sm dark:prose-invert max-w-none"><p node="[object Object]"><a href="https://arxiv.org/abs/2412.06257" node="[object Object]" class="text-accent hover:text-accent-dark underline-offset-2 underline" target="_blank" rel="noopener noreferrer">One paper</a> accepted in IEEE <a href="https://aixvr.tecnico.ulisboa.pt/" node="[object Object]" class="text-accent hover:text-accent-dark underline-offset-2 underline" target="_blank" rel="noopener noreferrer">AIxVR</a> 2025.</p></div></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2024-08</span><div class="text-sm text-neutral-700 prose prose-sm dark:prose-invert max-w-none"><p node="[object Object]">Joined CUHK CSE as a junior RA! üéâ</p></div></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2024-07</span><div class="text-sm text-neutral-700 prose prose-sm dark:prose-invert max-w-none"><p node="[object Object]"><a href="https://funqa-benchmark.github.io/" node="[object Object]" class="text-accent hover:text-accent-dark underline-offset-2 underline" target="_blank" rel="noopener noreferrer">FunQA</a> accepted to ECCV 2024.</p></div></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2024-06</span><div class="text-sm text-neutral-700 prose prose-sm dark:prose-invert max-w-none"><p node="[object Object]">Successfully graduated from BUPT and QMUL. üéâ</p></div></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2024-02</span><div class="text-sm text-neutral-700 prose prose-sm dark:prose-invert max-w-none"><p node="[object Object]"><a href="https://github.com/fesvhtr/CUVA" node="[object Object]" class="text-accent hover:text-accent-dark underline-offset-2 underline" target="_blank" rel="noopener noreferrer">CUVA</a> accepted to CVPR 2024.</p></div></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2023-12</span><div class="text-sm text-neutral-700 prose prose-sm dark:prose-invert max-w-none"><p node="[object Object]"><a href="https://github.com/fesvhtr/DocMSU" node="[object Object]" class="text-accent hover:text-accent-dark underline-offset-2 underline" target="_blank" rel="noopener noreferrer">DocMSU</a> accepted to AAAI 2024.</p></div></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2023-07</span><div class="text-sm text-neutral-700 prose prose-sm dark:prose-invert max-w-none"><p node="[object Object]">We released <a href="https://funqa-benchmark.github.io/" node="[object Object]" class="text-accent hover:text-accent-dark underline-offset-2 underline" target="_blank" rel="noopener noreferrer">FunQA Benchmark</a> and organized a <a href="https://iacc.pazhoulab-huangpu.com/contestdetail?id=64af50154a0ed647faca623a&amp;award=1,000,000" node="[object Object]" class="text-accent hover:text-accent-dark underline-offset-2 underline" target="_blank" rel="noopener noreferrer">competition</a> with 1 million ¬•.</p></div></div></div><div class="pointer-events-none absolute bottom-0 left-0 right-2 h-8 bg-gradient-to-t from-background to-transparent"></div></div></section></div></section></div></div><section id="featured_publications" class="mt-10 scroll-mt-24 space-y-8"><section style="opacity:0;transform:translateY(20px)"><h2 class="text-2xl font-serif font-bold text-primary mb-1">Selected Publications</h2><p class="mb-4 text-xs text-neutral-500 dark:text-neutral-500"><span class="font-semibold">*</span>: Equal Contribution,¬†<span class="font-semibold">‚Ä†</span>: Corresponding Author</p><div class="space-y-4"><div class="bg-white dark:bg-neutral-900 p-4 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-4"><div class="w-full md:w-1/2 lg:w-[45%] flex-shrink-0"><div class="relative h-32 md:h-36 rounded-lg overflow-hidden bg-transparent flex items-center justify-center"><img alt="EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning" loading="lazy" decoding="async" data-nimg="fill" class="object-contain" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/egohandicl.png"/></div></div><div class="flex-grow"><h3 class="text-base font-semibold text-primary mb-2 leading-snug"><a href="https://arxiv.org/abs/2601.19850" target="_blank" rel="noopener noreferrer" class="hover:underline underline-offset-2">EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning</a></h3><p class="text-sm text-neutral-600 dark:text-neutral-400 mb-2 leading-snug"><span><span class="font-semibold text-accent">Binzhu Xie</span><sup class="ml-0.5 text-accent" title="Co-first author">*</sup>, </span><span><span class="">Shi Qiu</span><sup class="ml-0.5 text-neutral-600 dark:text-neutral-400" title="Co-first author">*</sup><sup class="ml-0 text-neutral-600 dark:text-neutral-400">‚Ä†</sup>, </span><span><span class="">Sicheng Zhang</span>, </span><span><span class="">Yinqiao Wang</span>, </span><span><span class="">Hao Xu</span>, </span><span><span class="">Muzammal Naseer</span>, </span><span><span class="">Chi-Wing Fu</span>, </span><span><span class="">Pheng-Ann Heng</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-500 mb-2 leading-snug">International Conference on Learning Representations (ICLR)<!-- --> <!-- -->2026</p><div class="mt-2 flex flex-wrap gap-2 text-xs"><a href="https://arxiv.org/abs/2601.19850" target="_blank" rel="noopener noreferrer" class="px-2 py-1 rounded bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">[arxiv]</a><a href="https://github.com/Nicous20/EgoHandICL" target="_blank" rel="noopener noreferrer" class="px-2 py-1 rounded bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">[project¬†page]</a></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-4 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-4"><div class="w-full md:w-1/2 lg:w-[45%] flex-shrink-0"><div class="relative h-32 md:h-36 rounded-lg overflow-hidden bg-transparent flex items-center justify-center"><img alt="Generative Multi-Sensory Meditation: Exploring Immersive Depth and Activation in Virtual Reality" loading="lazy" decoding="async" data-nimg="fill" class="object-contain" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/meditation.png"/></div></div><div class="flex-grow"><h3 class="text-base font-semibold text-primary mb-2 leading-snug"><a href="https://arxiv.org/abs/2510.11830" target="_blank" rel="noopener noreferrer" class="hover:underline underline-offset-2">Generative Multi-Sensory Meditation: Exploring Immersive Depth and Activation in Virtual Reality</a></h3><p class="text-sm text-neutral-600 dark:text-neutral-400 mb-2 leading-snug"><span><span class="">Yuyang Jiang</span>, </span><span><span class="font-semibold text-accent">Binzhu Xie</span>, </span><span><span class="">Lina Xu</span>, </span><span><span class="">Xiaokang Lei</span>, </span><span><span class="">Shi Qiu</span>, </span><span><span class="">Luwen Yu</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">‚Ä†</sup>, </span><span><span class="">Pan Hui</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-500 mb-2 leading-snug">ACM Multimedia (ACM MM)<!-- --> <!-- -->2025</p><div class="mt-2 flex flex-wrap gap-2 text-xs"><a href="https://arxiv.org/abs/2510.11830" target="_blank" rel="noopener noreferrer" class="px-2 py-1 rounded bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">[arxiv]</a></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-4 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-4"><div class="w-full md:w-1/2 lg:w-[45%] flex-shrink-0"><div class="relative h-32 md:h-36 rounded-lg overflow-hidden bg-transparent flex items-center justify-center"><img alt="Trade-offs in Image Generation: How Do Different Dimensions Interact?" loading="lazy" decoding="async" data-nimg="fill" class="object-contain" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/trig.png"/></div></div><div class="flex-grow"><h3 class="text-base font-semibold text-primary mb-2 leading-snug"><a href="https://arxiv.org/abs/2507.22100" target="_blank" rel="noopener noreferrer" class="hover:underline underline-offset-2">Trade-offs in Image Generation: How Do Different Dimensions Interact?</a></h3><p class="text-sm text-neutral-600 dark:text-neutral-400 mb-2 leading-snug"><span><span class="">Sicheng Zhang</span><sup class="ml-0.5 text-neutral-600 dark:text-neutral-400" title="Co-first author">*</sup>, </span><span><span class="font-semibold text-accent">Binzhu Xie</span><sup class="ml-0.5 text-accent" title="Co-first author">*</sup>, </span><span><span class="">Zhonghao Yan</span><sup class="ml-0.5 text-neutral-600 dark:text-neutral-400" title="Co-first author">*</sup>, </span><span><span class="">Yuli Zhang</span>, </span><span><span class="">Donghao Zhou</span>, </span><span><span class="">Xiaofei Chen</span>, </span><span><span class="">Shi Qiu</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">‚Ä†</sup>, </span><span><span class="">Jiaqi Liu</span>, </span><span><span class="">Guoyang Xie</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">‚Ä†</sup>, </span><span><span class="">Zhichao Lu</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-500 mb-2 leading-snug">International Conference on Computer Vision (ICCV)<!-- --> <!-- -->2025</p><div class="mt-2 flex flex-wrap gap-2 text-xs"><a href="https://arxiv.org/abs/2507.22100" target="_blank" rel="noopener noreferrer" class="px-2 py-1 rounded bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">[arxiv]</a><a href="https://github.com/fesvhtr/TRIG" target="_blank" rel="noopener noreferrer" class="px-2 py-1 rounded bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">[project¬†page]</a></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-4 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-4"><div class="w-full md:w-1/2 lg:w-[45%] flex-shrink-0"><div class="relative h-32 md:h-36 rounded-lg overflow-hidden bg-transparent flex items-center justify-center"><img alt="EgoLife: Towards Egocentric Life Assistant" loading="lazy" decoding="async" data-nimg="fill" class="object-contain" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/egolife.png"/></div></div><div class="flex-grow"><h3 class="text-base font-semibold text-primary mb-2 leading-snug"><a href="https://arxiv.org/abs/2503.03803" target="_blank" rel="noopener noreferrer" class="hover:underline underline-offset-2">EgoLife: Towards Egocentric Life Assistant</a></h3><p class="text-sm text-neutral-600 dark:text-neutral-400 mb-2 leading-snug"><span><span class="">EgoLife Team</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-500 mb-2 leading-snug">Computer Vision and Pattern Recognition (CVPR)<!-- --> <!-- -->2025</p><div class="mt-2 flex flex-wrap gap-2 text-xs"><a href="https://arxiv.org/abs/2503.03803" target="_blank" rel="noopener noreferrer" class="px-2 py-1 rounded bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">[arxiv]</a><a href="https://egolife-ai.github.io/" target="_blank" rel="noopener noreferrer" class="px-2 py-1 rounded bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">[project¬†page]</a></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-4 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-4"><div class="w-full md:w-1/2 lg:w-[45%] flex-shrink-0"><div class="relative h-32 md:h-36 rounded-lg overflow-hidden bg-transparent flex items-center justify-center"><img alt="EchoTraffic: Enhancing traffic anomaly understanding with audio-visual insights" loading="lazy" decoding="async" data-nimg="fill" class="object-contain" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/echotraffic.png"/></div></div><div class="flex-grow"><h3 class="text-base font-semibold text-primary mb-2 leading-snug">EchoTraffic: Enhancing traffic anomaly understanding with audio-visual insights</h3><p class="text-sm text-neutral-600 dark:text-neutral-400 mb-2 leading-snug"><span><span class="">Zhenghao Xing</span><sup class="ml-0.5 text-neutral-600 dark:text-neutral-400" title="Co-first author">*</sup>, </span><span><span class="">Hao Chen</span><sup class="ml-0.5 text-neutral-600 dark:text-neutral-400" title="Co-first author">*</sup>, </span><span><span class="font-semibold text-accent">Binzhu Xie</span>, </span><span><span class="">Jiaqi Xu</span>, </span><span><span class="">Ziyu Guo</span>, </span><span><span class="">Xuemiao Xu</span>, </span><span><span class="">Jianye Hao</span>, </span><span><span class="">Chi-Wing Fu</span>, </span><span><span class="">Xiaowei Hu</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">‚Ä†</sup>, </span><span><span class="">Pheng-Ann Heng.</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-500 mb-2 leading-snug">Computer Vision and Pattern Recognition (CVPR)<!-- --> <!-- -->2025</p><div class="mt-2 flex flex-wrap gap-2 text-xs"><a href="https://openaccess.thecvf.com/content/CVPR2025/html/Xing_EchoTraffic_Enhancing_Traffic_Anomaly_Understanding_with_Audio-Visual_Insights_CVPR_2025_paper.html" target="_blank" rel="noopener noreferrer" class="px-2 py-1 rounded bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">[openreview]</a><a href="https://github.com/HarryHsing/EchoTraffic" target="_blank" rel="noopener noreferrer" class="px-2 py-1 rounded bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">[project¬†page]</a></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-4 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-4"><div class="w-full md:w-1/2 lg:w-[45%] flex-shrink-0"><div class="relative h-32 md:h-36 rounded-lg overflow-hidden bg-transparent flex items-center justify-center"><img alt="Creating Virtual Environments with 3D Gaussian Splatting: A Comparative Study" loading="lazy" decoding="async" data-nimg="fill" class="object-contain" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/vr3dgs.png"/></div></div><div class="flex-grow"><h3 class="text-base font-semibold text-primary mb-2 leading-snug"><a href="https://arxiv.org/abs/2501.09302" target="_blank" rel="noopener noreferrer" class="hover:underline underline-offset-2">Creating Virtual Environments with 3D Gaussian Splatting: A Comparative Study</a></h3><p class="text-sm text-neutral-600 dark:text-neutral-400 mb-2 leading-snug"><span><span class="">Shi Qiu</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">‚Ä†</sup>, </span><span><span class="font-semibold text-accent">Binzhu Xie</span>, </span><span><span class="">Qixuan Liu</span>, </span><span><span class="">Pheng-Ann Heng</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-500 mb-2 leading-snug">The IEEE Virtual Reality (IEEE VR) (Poster)<!-- --> <!-- -->2025</p><div class="mt-2 flex flex-wrap gap-2 text-xs"><a href="https://arxiv.org/abs/2501.09302" target="_blank" rel="noopener noreferrer" class="px-2 py-1 rounded bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">[arxiv]</a></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-4 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-4"><div class="w-full md:w-1/2 lg:w-[45%] flex-shrink-0"><div class="relative h-32 md:h-36 rounded-lg overflow-hidden bg-transparent flex items-center justify-center"><img alt="FunQA: Towards Surprising Video Comprehension" loading="lazy" decoding="async" data-nimg="fill" class="object-contain" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/funqa.png"/></div></div><div class="flex-grow"><h3 class="text-base font-semibold text-primary mb-2 leading-snug"><a href="https://arxiv.org/abs/2306.14899" target="_blank" rel="noopener noreferrer" class="hover:underline underline-offset-2">FunQA: Towards Surprising Video Comprehension</a></h3><p class="text-sm text-neutral-600 dark:text-neutral-400 mb-2 leading-snug"><span><span class="font-semibold text-accent">Binzhu Xie</span><sup class="ml-0.5 text-accent" title="Co-first author">*</sup>, </span><span><span class="">Sicheng Zhang</span><sup class="ml-0.5 text-neutral-600 dark:text-neutral-400" title="Co-first author">*</sup>, </span><span><span class="">Zitang Zhou</span><sup class="ml-0.5 text-neutral-600 dark:text-neutral-400" title="Co-first author">*</sup>, </span><span><span class="">Bo Li</span>, </span><span><span class="">Yuanhan Zhang</span>, </span><span><span class="">Jack Hessel</span>, </span><span><span class="">Jingkang Yang</span>, </span><span><span class="">Ziwei Liu</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">‚Ä†</sup></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-500 mb-2 leading-snug">European Conference on Computer Vision (ECCV)<!-- --> <!-- -->2024</p><div class="mt-2 flex flex-wrap gap-2 text-xs"><a href="https://arxiv.org/abs/2306.14899" target="_blank" rel="noopener noreferrer" class="px-2 py-1 rounded bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">[arxiv]</a><a href="https://funqa-benchmark.github.io/" target="_blank" rel="noopener noreferrer" class="px-2 py-1 rounded bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">[project¬†page]</a></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-4 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-4"><div class="w-full md:w-1/2 lg:w-[45%] flex-shrink-0"><div class="relative h-32 md:h-36 rounded-lg overflow-hidden bg-transparent flex items-center justify-center"><img alt="Uncovering What, Why and How: A Comprehensive Benchmark for Causation Understanding of Video Anomaly" loading="lazy" decoding="async" data-nimg="fill" class="object-contain" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/traf_anomaly.png"/></div></div><div class="flex-grow"><h3 class="text-base font-semibold text-primary mb-2 leading-snug"><a href="https://arxiv.org/abs/2405.00181" target="_blank" rel="noopener noreferrer" class="hover:underline underline-offset-2">Uncovering What, Why and How: A Comprehensive Benchmark for Causation Understanding of Video Anomaly</a></h3><p class="text-sm text-neutral-600 dark:text-neutral-400 mb-2 leading-snug"><span><span class="">Hang Du</span><sup class="ml-0.5 text-neutral-600 dark:text-neutral-400" title="Co-first author">*</sup>, </span><span><span class="">Sicheng Zhang</span><sup class="ml-0.5 text-neutral-600 dark:text-neutral-400" title="Co-first author">*</sup>, </span><span><span class="font-semibold text-accent">Binzhu Xie</span><sup class="ml-0.5 text-accent" title="Co-first author">*</sup>, </span><span><span class="">Guoshun Nan</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">‚Ä†</sup>, </span><span><span class="">Jiayang Zhang</span>, </span><span><span class="">Junrui Xu</span>, </span><span><span class="">Hangyu Liu</span>, </span><span><span class="">Sicong Leng</span>, </span><span><span class="">Jiangming Liu</span>, </span><span><span class="">Hehe Fan</span>, </span><span><span class="">Dajiu Huang</span>, </span><span><span class="">Jing Feng</span>, </span><span><span class="">Linli Chen</span>, </span><span><span class="">Can Zhang</span>, </span><span><span class="">Xuhuan Li</span>, </span><span><span class="">Hao Zhang</span>, </span><span><span class="">Jianhang Chen</span>, </span><span><span class="">Qimei Cui</span>, </span><span><span class="">Xiaofeng Tao</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-500 mb-2 leading-snug">Computer Vision and Pattern Recognition (CVPR)<!-- --> <!-- -->2024</p><div class="mt-2 flex flex-wrap gap-2 text-xs"><a href="https://arxiv.org/abs/2405.00181" target="_blank" rel="noopener noreferrer" class="px-2 py-1 rounded bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">[arxiv]</a><a href="https://github.com/fesvhtr/CUVA" target="_blank" rel="noopener noreferrer" class="px-2 py-1 rounded bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">[project¬†page]</a></div></div></div></div></div></section></section><div class="mt-10 space-y-8"><section id="services" class="scroll-mt-24 space-y-8"><div style="opacity:0;transform:translateY(20px)"><div class="mb-4"><h1 class="text-2xl font-serif font-bold text-primary mb-4">Services</h1></div><div class="grid gap-4"><div class="bg-white dark:bg-neutral-900 p-4 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-lg transition-all duration-200 hover:scale-[1.01]" style="opacity:0;transform:translateY(20px)"><div class="flex justify-between items-start mb-2"><h3 class="text-lg font-semibold text-primary">Conference Reviewer</h3></div><p class="text-sm text-neutral-600 dark:text-neutral-500 leading-relaxed">CVPR (2026), ICLR (2026), AAAI (2026), ACM MM (2025)., ACL ARR (2025, 2026).</p></div></div></div></section></div></div><!--$--><!--/$--><!--$--><!--/$--></main><footer class="border-t border-neutral-200/50 bg-neutral-50/50 dark:bg-neutral-900/50 dark:border-neutral-700/50"><div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-6"><div class="flex flex-col sm:flex-row justify-between items-center gap-2"><p class="text-xs text-neutral-500">Last updated: <!-- -->January 27, 2026</p><p class="text-xs text-neutral-500 flex items-center"><a href="https://github.com/xyjoey/PRISM" target="_blank" rel="noopener noreferrer">Built with PRISM</a><span class="ml-2">üöÄ</span></p></div></div></footer></div><script src="/_next/static/chunks/webpack-679b75d1c4c2027c.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[3719,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"473\",\"static/chunks/473-8ffb501d6ec7a482.js\",\"177\",\"static/chunks/app/layout-2cd749d8c5f06c63.js\"],\"ThemeProvider\"]\n3:I[768,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"473\",\"static/chunks/473-8ffb501d6ec7a482.js\",\"177\",\"static/chunks/app/layout-2cd749d8c5f06c63.js\"],\"default\"]\n4:I[7555,[],\"\"]\n5:I[1295,[],\"\"]\n6:I[2548,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"473\",\"static/chunks/473-8ffb501d6ec7a482.js\",\"177\",\"static/chunks/app/layout-2cd749d8c5f06c63.js\"],\"default\"]\n7:I[7437,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"178\",\"static/chunks/178-595a94b9af1e67b5.js\",\"748\",\"static/chunks/748-1f3129a1e6365cf9.js\",\"974\",\"static/chunks/app/page-4a023a9e3aac6a3b.js\"],\"default\"]\n8:I[9507,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"178\",\"static/chunks/178-595a94b9af1e67b5.js\",\"748\",\"static/chunks/748-1f3129a1e6365cf9.js\",\"974\",\"static/chunks/app/page-4a023a9e3aac6a3b.js\"],\"default\"]\n9:I[1990,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"178\",\"static/chunks/178-595a94b9af1e67b5.js\",\"748\",\"static/chunks/748-1f3129a1e6365cf9.js\",\"974\",\"static/chunks/app/page-4a023a9e3aac6a3b.js\"],\"default\"]\na:I[5218,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"178\",\"static/chunks/178-595a94b9af1e67b5.js\",\"748\",\"static/chunks/748-1f3129a1e6365cf9.js\",\"974\",\"static/chunks/app/page-4a023a9e3aac6a3b.js\"],\"default\"]\nb:I[3684,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"178\",\"static/chunks/178-595a94b9af1e67b5.js\",\"748\",\"static/chunks/748-1f3129a1e6365cf9.js\",\"974\",\"static/chunks/app/page-4a023a9e3aac6a3b.js\"],\"default\"]\nc:I[9665,[],\"MetadataBoundary\"]\ne:I[9665,[],\"OutletBoundary\"]\n11:I[4911,[],\"AsyncMetadataOutlet\"]\n13:I[9665,[],\"ViewportBoundary\"]\n15:I[6614,[],\"\"]\n:HL[\"/_next/static/css/bd781d6ed6b0325b.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"ASZUu2HOLcen9GjjaI7Dv\",\"p\":\"\",\"c\":[\"\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"__PAGE__\",{}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/bd781d6ed6b0325b.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"scroll-smooth\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"icon\",\"href\":\"/favicon.svg\",\"type\":\"image/svg+xml\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://google-fonts.jialeliu.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://google-fonts.jialeliu.com\",\"crossOrigin\":\"\"}],[\"$\",\"link\",null,{\"rel\":\"preload\",\"as\":\"style\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}],[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"id\":\"gfonts-css\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\",\"media\":\"print\"}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              (function(){\\n                var l = document.getElementById('gfonts-css');\\n                if (!l) return;\\n                if (l.media !== 'all') {\\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\\n                }\\n              })();\\n            \"}}],[\"$\",\"noscript\",null,{\"children\":[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}]}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              try {\\n                const theme = localStorage.getItem('theme-storage');\\n                const parsed = theme ? JSON.parse(theme) : null;\\n                const setting = parsed?.state?.theme || 'system';\\n                const prefersDark = typeof window !== 'undefined' \u0026\u0026 window.matchMedia \u0026\u0026 window.matchMedia('(prefers-color-scheme: dark)').matches;\\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\\n                var root = document.documentElement;\\n                root.classList.add(effective);\\n                root.setAttribute('data-theme', effective);\\n              } catch (e) {\\n                var root = document.documentElement;\\n                root.classList.add('light');\\n                root.setAttribute('data-theme', 'light');\\n              }\\n            \"}}]]}],[\"$\",\"body\",null,{\"className\":\"font-sans antialiased\",\"children\":[\"$\",\"$L2\",null,{\"children\":[[\"$\",\"$L3\",null,{\"items\":[{\"title\":\"About\",\"type\":\"page\",\"target\":\"about\",\"href\":\"/\"},{\"title\":\"News\",\"type\":\"link\",\"target\":\"news\",\"href\":\"/#news\"},{\"title\":\"Selected Publications\",\"type\":\"link\",\"target\":\"featured_publications\",\"href\":\"/#featured_publications\"},{\"title\":\"Services\",\"type\":\"page\",\"target\":\"services\",\"href\":\"/services\"}],\"siteTitle\":\"Binzhu Xie\",\"enableOnePageMode\":true}],[\"$\",\"main\",null,{\"className\":\"min-h-screen pt-16 lg:pt-20\",\"children\":[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],[\"$\",\"$L6\",null,{\"lastUpdated\":\"January 27, 2026\"}]]}]}]]}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"div\",null,{\"className\":\"max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen\",\"children\":[[\"$\",\"div\",null,{\"className\":\"grid grid-cols-1 lg:grid-cols-3 gap-12\",\"children\":[[\"$\",\"div\",null,{\"className\":\"lg:col-span-1\",\"children\":[\"$\",\"$L7\",null,{\"author\":{\"name\":\"Binzhu Xie\",\"title\":\"Ph.D. Student @ CUHK\",\"institution\":\"The Chinese University of Hong Kong.\",\"avatar\":\"/bio.jpg\"},\"social\":{\"emails\":[\"binzhuxie@gmail.com\",\"bzxie@cse.cuhk.edu.hk\"],\"location\":\"Hong Kong, SAR\",\"location_url\":\"https://maps.app.goo.gl/wCrwjKLDuoCeU3V39\",\"location_details\":[\"Academic Building No.1\",\"The Chinese University of Hong Kong\",\"ShaTin, Hong Kong SAR\"],\"google_scholar\":\"https://scholar.google.com/citations?user=yFREVP0AAAAJ\u0026hl=en\",\"github\":\"https://github.com/Nicous20\",\"linkedin\":\"https://www.linkedin.com/in/binzhu-xie-bb2b04298/\"},\"features\":{\"enable_likes\":true,\"enable_one_page_mode\":true},\"researchInterests\":[\"3D Vision (highly related to XR and HCI)\",\"Visual Multimodally Understanding\",\"AIGC related topic\"]}]}],[\"$\",\"div\",null,{\"className\":\"lg:col-span-2 space-y-8\",\"children\":[\"$\",\"section\",\"about\",{\"id\":\"about\",\"className\":\"scroll-mt-24 space-y-8\",\"children\":[[\"$\",\"$L8\",\"about\",{\"content\":\"Binzhu Xie (Ë∞¢Êª®Á´π) is a first year PhD student at the [Department of Computer Science and Engineering](https://www.cse.cuhk.edu.hk/), [The Chinese University of Hong Kong (CUHK)](https://www.cuhk.edu.hk/chinese/index.html), advised by [Prof. Shi QIU](https://shiqiu0419.github.io/) at the CUHK‚Äôs Institute of Medical Intelligence and XR, led by [Prof. Pheng Ann Heng](https://www.cse.cuhk.edu.hk/~pheng/). Previously, Binzhu Xie obtained his bachelor in 2020 from a [joint program](https://www.qmul.ac.uk/global/partnerships/jointprogrammes/bupt/) between [BUPT](https://www.bupt.edu.cn/) and [QMUL](https://www.qmul.ac.uk/), advised by [Prof. Guoshun Nan](https://scholar.google.com/citations?user=uSykWkMAAAAJ\u0026hl=en). Since 2023, He has been fortunate to collaborate with [MMLab@NTU](https://www.mmlab-ntu.com/index.html), working closely with [Dr. Jingkang Yang](https://jingkang50.github.io/) under the supervision of [Prof. Ziwei Liu](https://liuziwei7.github.io/).\\n\",\"title\":\"About\"}],[\"$\",\"div\",\"news\",{\"id\":\"news\",\"children\":[\"$\",\"$L9\",null,{\"items\":[{\"date\":\"2026-01\",\"content\":\"[EgoHandICL](https://arxiv.org/abs/2601.19850) accepted to ICLR 2026. \"},{\"date\":\"2026-01\",\"content\":\"Joined CUHK CSE as a PhD student! üéâ \"},{\"date\":\"2025-07\",\"content\":\"[One paper](https://arxiv.org/abs/2510.11830) accepted to ACM MM 2025.\"},{\"date\":\"2025-06\",\"content\":\"[TRIG](https://arxiv.org/abs/2507.22100) accepted to ICCV 2025.\"},{\"date\":\"2025-03\",\"content\":\"Two papers accepted to CVPR 2025. [EgoLife](https://egolife-ai.github.io/) is fantastic! üï∂Ô∏è\"},{\"date\":\"2025-01\",\"content\":\"[One work](https://arxiv.org/abs/2501.09302) accepted as IEEE VR 2025 Posters.\"},{\"date\":\"2024-11\",\"content\":\"[One paper](https://arxiv.org/abs/2412.06257) accepted in IEEE [AIxVR](https://aixvr.tecnico.ulisboa.pt/) 2025.\"},{\"date\":\"2024-08\",\"content\":\"Joined CUHK CSE as a junior RA! üéâ \"},{\"date\":\"2024-07\",\"content\":\"[FunQA](https://funqa-benchmark.github.io/) accepted to ECCV 2024.\"},{\"date\":\"2024-06\",\"content\":\"Successfully graduated from BUPT and QMUL. üéâ \"},{\"date\":\"2024-02\",\"content\":\"[CUVA](https://github.com/fesvhtr/CUVA) accepted to CVPR 2024.\"},{\"date\":\"2023-12\",\"content\":\"[DocMSU](https://github.com/fesvhtr/DocMSU) accepted to AAAI 2024.\"},{\"date\":\"2023-07\",\"content\":\"We released [FunQA Benchmark](https://funqa-benchmark.github.io/) and organized a [competition](https://iacc.pazhoulab-huangpu.com/contestdetail?id=64af50154a0ed647faca623a\u0026award=1,000,000) with 1 million ¬•.\"}],\"title\":\"News\"}]}]]}]}]]}],[\"$\",\"section\",null,{\"id\":\"featured_publications\",\"className\":\"mt-10 scroll-mt-24 space-y-8\",\"children\":[[\"$\",\"$La\",\"featured_publications\",{\"publications\":[{\"id\":\"xie2026egohandiclegocentric3dhand\",\"title\":\"EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning\",\"authors\":[{\"name\":\"Binzhu Xie\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":true},{\"name\":\"Shi Qiu\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":true},{\"name\":\"Sicheng Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yinqiao Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Hao Xu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Muzammal Naseer\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chi-Wing Fu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Pheng-Ann Heng\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2026,\"type\":\"journal\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:1:props:children:0:props:publications:0:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"International Conference on Learning Representations (ICLR)\",\"conference\":\"\",\"url\":\"https://arxiv.org/abs/2601.19850\",\"project\":\"https://github.com/Nicous20/EgoHandICL\",\"abstract\":\"\",\"description\":\"\",\"selected\":true,\"preview\":\"egohandicl.png\",\"bibtex\":\"@article{xie2026egohandiclegocentric3dhand,\\n  url = {https://arxiv.org/abs/2601.19850},\\n  title = {EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning},\\n  author = {Binzhu Xie and Shi Qiu and Sicheng Zhang and Yinqiao Wang and Hao Xu and Muzammal Naseer and Chi-Wing Fu and Pheng-Ann Heng},\\n  year = {2026},\\n  journal = {International Conference on Learning Representations (ICLR)}\\n}\"},{\"id\":\"jiang2025generative\",\"title\":\"Generative Multi-Sensory Meditation: Exploring Immersive Depth and Activation in Virtual Reality\",\"authors\":[{\"name\":\"Yuyang Jiang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Binzhu Xie\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Lina Xu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiaokang Lei\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Shi Qiu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Luwen Yu\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Pan Hui\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"type\":\"journal\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:1:props:children:0:props:publications:1:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"ACM Multimedia (ACM MM)\",\"conference\":\"\",\"url\":\"https://arxiv.org/abs/2510.11830\",\"abstract\":\"\",\"description\":\"\",\"selected\":true,\"preview\":\"meditation.png\",\"bibtex\":\"@article{jiang2025generative,\\n  url = {https://arxiv.org/abs/2510.11830},\\n  title = {Generative Multi-Sensory Meditation: Exploring Immersive Depth and Activation in Virtual Reality},\\n  author = {Yuyang Jiang and Binzhu Xie and Lina Xu and Xiaokang Lei and Shi Qiu and Luwen Yu and Pan Hui},\\n  year = {2025},\\n  journal = {ACM Multimedia (ACM MM)}\\n}\"},{\"id\":\"zhang2025tradeoffsimagegenerationdifferent\",\"title\":\"Trade-offs in Image Generation: How Do Different Dimensions Interact?\",\"authors\":[{\"name\":\"Sicheng Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":true},{\"name\":\"Binzhu Xie\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":true},{\"name\":\"Zhonghao Yan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":true},{\"name\":\"Yuli Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Donghao Zhou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiaofei Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Shi Qiu\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Jiaqi Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Guoyang Xie\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Zhichao Lu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"type\":\"journal\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:1:props:children:0:props:publications:2:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"International Conference on Computer Vision (ICCV)\",\"conference\":\"\",\"url\":\"https://arxiv.org/abs/2507.22100\",\"project\":\"https://github.com/fesvhtr/TRIG\",\"abstract\":\"\",\"description\":\"\",\"selected\":true,\"preview\":\"trig.png\",\"bibtex\":\"@article{zhang2025tradeoffsimagegenerationdifferent,\\n  url = {https://arxiv.org/abs/2507.22100},\\n  title = {Trade-offs in Image Generation: How Do Different Dimensions Interact?},\\n  author = {Sicheng Zhang and Binzhu Xie and Zhonghao Yan and Yuli Zhang and Donghao Zhou and Xiaofei Chen and Shi Qiu and Jiaqi Liu and Guoyang Xie and Zhichao Lu},\\n  year = {2025},\\n  journal = {International Conference on Computer Vision (ICCV)}\\n}\"},{\"id\":\"yang2025egolifeegocentriclifeassistant\",\"title\":\"EgoLife: Towards Egocentric Life Assistant\",\"authors\":[{\"name\":\"EgoLife Team\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"type\":\"journal\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:1:props:children:0:props:publications:3:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"Computer Vision and Pattern Recognition (CVPR)\",\"conference\":\"\",\"url\":\"https://arxiv.org/abs/2503.03803\",\"project\":\"https://egolife-ai.github.io/\",\"abstract\":\"\",\"description\":\"\",\"selected\":true,\"preview\":\"egolife.png\",\"bibtex\":\"@article{yang2025egolifeegocentriclifeassistant,\\n  url = {https://arxiv.org/abs/2503.03803},\\n  title = {EgoLife: Towards Egocentric Life Assistant},\\n  author = {EgoLife Team},\\n  year = {2025},\\n  journal = {Computer Vision and Pattern Recognition (CVPR)}\\n}\"},{\"id\":\"xing2025echo\",\"title\":\"EchoTraffic: Enhancing traffic anomaly understanding with audio-visual insights\",\"authors\":[{\"name\":\"Zhenghao Xing\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":true},{\"name\":\"Hao Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":true},{\"name\":\"Binzhu Xie\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jiaqi Xu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ziyu Guo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xuemiao Xu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jianye Hao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chi-Wing Fu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiaowei Hu\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Pheng-Ann Heng.\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"type\":\"journal\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:1:props:children:0:props:publications:4:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"Computer Vision and Pattern Recognition (CVPR)\",\"conference\":\"\",\"openreview\":\"https://openaccess.thecvf.com/content/CVPR2025/html/Xing_EchoTraffic_Enhancing_Traffic_Anomaly_Understanding_with_Audio-Visual_Insights_CVPR_2025_paper.html\",\"project\":\"https://github.com/HarryHsing/EchoTraffic\",\"abstract\":\"\",\"description\":\"\",\"selected\":true,\"preview\":\"echotraffic.png\",\"bibtex\":\"@article{xing2025echo,\\n  title = {EchoTraffic: Enhancing traffic anomaly understanding with audio-visual insights},\\n  author = {Zhenghao Xing and Hao Chen and Binzhu Xie and Jiaqi Xu and Ziyu Guo and Xuemiao Xu and Jianye Hao and Chi-Wing Fu and Xiaowei Hu and Pheng-Ann Heng.},\\n  year = {2025},\\n  journal = {Computer Vision and Pattern Recognition (CVPR)}\\n}\"},{\"id\":\"qiu2025creatingvirtualenvironments3d\",\"title\":\"Creating Virtual Environments with 3D Gaussian Splatting: A Comparative Study\",\"authors\":[{\"name\":\"Shi Qiu\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Binzhu Xie\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Qixuan Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Pheng-Ann Heng\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"type\":\"journal\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:1:props:children:0:props:publications:5:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"The IEEE Virtual Reality (IEEE VR) (Poster)\",\"conference\":\"\",\"url\":\"https://arxiv.org/abs/2501.09302\",\"abstract\":\"\",\"description\":\"\",\"selected\":true,\"preview\":\"vr3dgs.png\",\"bibtex\":\"@article{qiu2025creatingvirtualenvironments3d,\\n  url = {https://arxiv.org/abs/2501.09302},\\n  title = {Creating Virtual Environments with 3D Gaussian Splatting: A Comparative Study},\\n  author = {Shi Qiu and Binzhu Xie and Qixuan Liu and Pheng-Ann Heng},\\n  year = {2025},\\n  journal = {The IEEE Virtual Reality (IEEE VR) (Poster)}\\n}\"},{\"id\":\"xie2024funqasurprisingvideocomprehension\",\"title\":\"FunQA: Towards Surprising Video Comprehension\",\"authors\":[{\"name\":\"Binzhu Xie\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":true},{\"name\":\"Sicheng Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":true},{\"name\":\"Zitang Zhou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":true},{\"name\":\"Bo Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yuanhan Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jack Hessel\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jingkang Yang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ziwei Liu\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false}],\"year\":2024,\"type\":\"preprint\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:1:props:children:0:props:publications:6:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"European Conference on Computer Vision (ECCV)\",\"conference\":\"\",\"url\":\"https://arxiv.org/abs/2306.14899\",\"project\":\"https://funqa-benchmark.github.io/\",\"abstract\":\"\",\"description\":\"\",\"selected\":true,\"preview\":\"funqa.png\",\"bibtex\":\"@misc{xie2024funqasurprisingvideocomprehension,\\n  url = {https://arxiv.org/abs/2306.14899},\\n  title = {FunQA: Towards Surprising Video Comprehension},\\n  author = {Binzhu Xie and Sicheng Zhang and Zitang Zhou and Bo Li and Yuanhan Zhang and Jack Hessel and Jingkang Yang and Ziwei Liu},\\n  year = {2024},\\n  journal = {European Conference on Computer Vision (ECCV)}\\n}\"},{\"id\":\"du2024uncoveringwhathowcomprehensive\",\"title\":\"Uncovering What, Why and How: A Comprehensive Benchmark for Causation Understanding of Video Anomaly\",\"authors\":[{\"name\":\"Hang Du\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":true},{\"name\":\"Sicheng Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":true},{\"name\":\"Binzhu Xie\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":true},{\"name\":\"Guoshun Nan\",\"isHighlighted\":false,\"isCorresponding\":true,\"isCoAuthor\":false},{\"name\":\"Jiayang Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Junrui Xu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Hangyu Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Sicong Leng\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jiangming Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Hehe Fan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Dajiu Huang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jing Feng\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Linli Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Can Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xuhuan Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Hao Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jianhang Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Qimei Cui\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xiaofeng Tao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2024,\"type\":\"journal\",\"status\":\"published\",\"tags\":[],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:1:props:children:0:props:publications:7:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"Computer Vision and Pattern Recognition (CVPR)\",\"conference\":\"\",\"url\":\"https://arxiv.org/abs/2405.00181\",\"project\":\"https://github.com/fesvhtr/CUVA\",\"abstract\":\"\",\"description\":\"\",\"selected\":true,\"preview\":\"traf_anomaly.png\",\"bibtex\":\"@article{du2024uncoveringwhathowcomprehensive,\\n  url = {https://arxiv.org/abs/2405.00181},\\n  title = {Uncovering What, Why and How: A Comprehensive Benchmark for Causation Understanding of Video Anomaly},\\n  author = {Hang Du and Sicheng Zhang and Binzhu Xie and Guoshun Nan and Jiayang Zhang and Junrui Xu and Hangyu Liu and Sicong Leng and Jiangming Liu and Hehe Fan and Dajiu Huang and Jing Feng and Linli Chen and Can Zhang and Xuhuan Li and Hao Zhang and Jianhang Chen and Qimei Cui and Xiaofeng Tao},\\n  year = {2024},\\n  journal = {Computer Vision and Pattern Recognition (CVPR)}\\n}\"}],\"title\":\"Selected Publications\",\"enableOnePageMode\":true}]]}],[\"$\",\"div\",null,{\"className\":\"mt-10 space-y-8\",\"children\":[[\"$\",\"section\",\"services\",{\"id\":\"services\",\"className\":\"scroll-mt-24 space-y-8\",\"children\":[false,false,[\"$\",\"$Lb\",null,{\"config\":{\"type\":\"card\",\"title\":\"Services\",\"items\":[{\"title\":\"Conference Reviewer\",\"content\":\"CVPR (2026), ICLR (2026), AAAI (2026), ACM MM (2025)., ACL ARR (2025, 2026).\"}]},\"embedded\":true}]]}]]}]]}],[\"$\",\"$Lc\",null,{\"children\":\"$Ld\"}],null,[\"$\",\"$Le\",null,{\"children\":[\"$Lf\",\"$L10\",[\"$\",\"$L11\",null,{\"promise\":\"$@12\"}]]}]]}],{},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"mO5d1Jw92o04dw98nIy3M\",{\"children\":[[\"$\",\"$L13\",null,{\"children\":\"$L14\"}],null]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$15\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"16:\"$Sreact.suspense\"\n17:I[4911,[],\"AsyncMetadata\"]\nd:[\"$\",\"$16\",null,{\"fallback\":null,\"children\":[\"$\",\"$L17\",null,{\"promise\":\"$@18\"}]}]\n"])</script><script>self.__next_f.push([1,"10:null\n"])</script><script>self.__next_f.push([1,"14:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\nf:null\n"])</script><script>self.__next_f.push([1,"18:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Binzhu Xie\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"PhD student at The Chinese University of Hong Kong.\"}],[\"$\",\"meta\",\"2\",{\"name\":\"author\",\"content\":\"Binzhu Xie\"}],[\"$\",\"meta\",\"3\",{\"name\":\"keywords\",\"content\":\"Binzhu Xie,PhD,Research,The Chinese University of Hong Kong.\"}],[\"$\",\"meta\",\"4\",{\"name\":\"creator\",\"content\":\"Binzhu Xie\"}],[\"$\",\"meta\",\"5\",{\"name\":\"publisher\",\"content\":\"Binzhu Xie\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:title\",\"content\":\"Binzhu Xie\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:description\",\"content\":\"PhD student at The Chinese University of Hong Kong.\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:site_name\",\"content\":\"Binzhu Xie's Academic Website\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:title\",\"content\":\"Binzhu Xie\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:description\",\"content\":\"PhD student at The Chinese University of Hong Kong.\"}],[\"$\",\"link\",\"14\",{\"rel\":\"icon\",\"href\":\"/favicon.svg\"}]],\"error\":null,\"digest\":\"$undefined\"}\n12:{\"metadata\":\"$18:metadata\",\"error\":null,\"digest\":\"$undefined\"}\n"])</script></body></html>